16:23:21.101 [I] Running on: iris-hgx-2.stanford.edu                                              (3337674:train.py:251)
16:23:24.277 [I] Created BasePyTreeCheckpointHandler: use_ocdbt=True, use_zarr3=False, pytree_metadata_options=PyTreeMetadataOptions(support_rich_types=False), array_metadata_store=<orbax.checkpoint._src.metadata.array_metadata_store.Store object at 0x1547573d2990> (3337674:base_pytree_checkpoint_handler.py:334)
16:23:24.277 [I] Created BasePyTreeCheckpointHandler: use_ocdbt=True, use_zarr3=False, pytree_metadata_options=PyTreeMetadataOptions(support_rich_types=False), array_metadata_store=<orbax.checkpoint._src.metadata.array_metadata_store.Store object at 0x1547573d2990> (3337674:base_pytree_checkpoint_handler.py:334)
16:23:24.278 [I] [thread=MainThread] Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID. (3337674:multihost.py:390)
16:23:24.279 [I] [process=0][thread=MainThread] CheckpointManager init: checkpointers=None, item_names=None, item_handlers={'assets': <openpi.training.checkpoints.CallbackHandler object at 0x15465614a550>, 'train_state': <orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x15465623c110>, 'params': <orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x154b183ae750>}, handler_registry=None (3337674:checkpoint_manager.py:620)
16:23:24.280 [I] Deferred registration for item: "assets". Adding handler `<openpi.training.checkpoints.CallbackHandler object at 0x15465614a550>` for item "assets" and save args `<class 'openpi.training.checkpoints.CallbackSave'>` and restore args `<class 'openpi.training.checkpoints.CallbackRestore'>` to `_handler_registry`. (3337674:composite_checkpoint_handler.py:234)
16:23:24.280 [I] Deferred registration for item: "train_state". Adding handler `<orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x15465623c110>` for item "train_state" and save args `<class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeSaveArgs'>` and restore args `<class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeRestoreArgs'>` to `_handler_registry`. (3337674:composite_checkpoint_handler.py:234)
16:23:24.280 [I] Deferred registration for item: "params". Adding handler `<orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x154b183ae750>` for item "params" and save args `<class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeSaveArgs'>` and restore args `<class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeRestoreArgs'>` to `_handler_registry`. (3337674:composite_checkpoint_handler.py:234)
16:23:24.280 [I] Deferred registration for item: "metrics". Adding handler `<orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonCheckpointHandler object at 0x154656149e50>` for item "metrics" and save args `<class 'orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonSaveArgs'>` and restore args `<class 'orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonRestoreArgs'>` to `_handler_registry`. (3337674:composite_checkpoint_handler.py:234)
16:23:24.280 [I] Initialized registry DefaultCheckpointHandlerRegistry({('assets', <class 'openpi.training.checkpoints.CallbackSave'>): <openpi.training.checkpoints.CallbackHandler object at 0x15465614a550>, ('assets', <class 'openpi.training.checkpoints.CallbackRestore'>): <openpi.training.checkpoints.CallbackHandler object at 0x15465614a550>, ('train_state', <class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeSaveArgs'>): <orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x15465623c110>, ('train_state', <class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeRestoreArgs'>): <orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x15465623c110>, ('params', <class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeSaveArgs'>): <orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x154b183ae750>, ('params', <class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeRestoreArgs'>): <orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x154b183ae750>, ('metrics', <class 'orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonSaveArgs'>): <orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonCheckpointHandler object at 0x154656149e50>, ('metrics', <class 'orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonRestoreArgs'>): <orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonCheckpointHandler object at 0x154656149e50>}). (3337674:composite_checkpoint_handler.py:502)
16:23:24.281 [I] orbax-checkpoint version: 0.11.13                                                (3337674:abstract_checkpointer.py:35)
16:23:24.281 [I] [process=0][thread=MainThread] Using barrier_sync_fn: <function get_barrier_sync_fn.<locals>.<lambda> at 0x1546562a19e0> timeout: 7200 secs and primary_host=0 for async checkpoint writes (3337674:async_checkpointer.py:170)
16:23:24.292 [I] Read Metadata={'item_handlers': {'assets': 'openpi.training.checkpoints.CallbackHandler', 'params': 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler', 'train_state': 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler'}, 'metrics': {}, 'performance_metrics': {}, 'init_timestamp_nsecs': 1752705303684065497, 'commit_timestamp_nsecs': 1752705339214941662, 'custom_metadata': {}} from /iris/u/jrpan/openpi/checkpoints/pi0_fast_robomemory_low_mem_finetune/lora_10k_0716/2000/_CHECKPOINT_METADATA (3337674:checkpoint.py:226)
16:23:24.293 [W] Missing metrics for step 2000                                                    (3337674:checkpoint_manager.py:1654)
16:23:24.294 [E] File /iris/u/jrpan/openpi/checkpoints/pi0_fast_robomemory_low_mem_finetune/lora_10k_0716/2000/metrics/metrics not found. (3337674:checkpoint_manager.py:1655)
16:23:24.294 [I] Found 1 checkpoint steps in /iris/u/jrpan/openpi/checkpoints/pi0_fast_robomemory_low_mem_finetune/lora_10k_0716 (3337674:checkpoint_manager.py:1701)
16:23:24.296 [I] [process=0][thread=MainThread] CheckpointManager created,  primary_host=0, CheckpointManagerOptions=CheckpointManagerOptions(save_interval_steps=1, max_to_keep=1, keep_time_interval=None, keep_period=1000, should_keep_fn=None, best_fn=None, best_mode='max', keep_checkpoints_without_metrics=True, step_prefix=None, step_format_fixed_length=None, step_name_format=None, create=False, cleanup_tmp_directories=False, save_on_steps=frozenset(), single_host_load_and_broadcast=False, todelete_subdir=None, enable_background_delete=False, read_only=False, enable_async_checkpointing=True, async_options=AsyncOptions(timeout_secs=7200, barrier_sync_fn=None, post_finalization_callback=None, create_directories_asynchronously=True), multiprocessing_options=MultiprocessingOptions(primary_host=0, active_processes=None, barrier_sync_key_prefix=None), should_save_fn=None, file_options=FileOptions(path_permission_mode=None), save_root_metadata=True, temporary_path_class=None, save_decision_policy=None, prevent_write_metrics=False), root_directory=/iris/u/jrpan/openpi/checkpoints/pi0_fast_robomemory_low_mem_finetune/lora_10k_0716: <orbax.checkpoint.checkpoint_manager.CheckpointManager object at 0x15465614ba50> (3337674:checkpoint_manager.py:801)
2025/07/16 16:23:24 ERROR failed to get logger path error="error opening log file: open /sailhome/jrpan/.cache/wandb/logs/core-debug-20250716_162324.log: disk quota exceeded"
2025/07/16 16:23:24 INFO server is running addr=127.0.0.1:45723
2025/07/16 16:23:24 INFO Will exit if parent process dies. ppid=3337674
2025/07/16 16:23:24 INFO connection: ManageConnectionData: new connection created id=127.0.0.1:57224
wandb: Currently logged in as: jpan00 (robo-memory) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025/07/16 16:23:24 INFO handleInformInit: received streamId=29s9d6jz id=127.0.0.1:57224
2025/07/16 16:23:25 INFO handleInformInit: stream started streamId=29s9d6jz id=127.0.0.1:57224
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /iris/u/jrpan/openpi/wandb/run-20250716_162324-29s9d6jz
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run lora_10k_0716
wandb: ⭐️ View project at https://wandb.ai/robo-memory/openpi
wandb: 🚀 View run at https://wandb.ai/robo-memory/openpi/runs/29s9d6jz
Some kwargs in processor config are unused and will not have any effect: vocab_size, scale, time_horizon, min_token, action_dim. 
Exception in thread Thread-1 (inner):
Traceback (most recent call last):
  File "/sailhome/jrpan/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/threading.py", line 1045, in _bootstrap_inner
    self.run()
  File "/sailhome/jrpan/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/threading.py", line 982, in run
    self._target(*self._args, **self._kwargs)
  File "/iris/u/jrpan/openpi/scripts/train.py", line 16, in inner
    posix.rename(f'{dir_prefix}/memory.prof.new', f'{dir_prefix}/memory.prof')  # atomic
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/dev/shm/memory.prof.new' -> '/dev/shm/memory.prof'
Some kwargs in processor config are unused and will not have any effect: vocab_size, scale, time_horizon, min_token, action_dim. 
16:23:29.755 [I] Loaded norm stats from /iris/u/jrpan/openpi/assets/pi0_fast_robomemory_low_mem_finetune/jennypan00/pi0_fast_ft_droid_lerobot (3337674:config.py:170)
Some kwargs in processor config are unused and will not have any effect: vocab_size, scale, time_horizon, min_token, action_dim. 
Some kwargs in processor config are unused and will not have any effect: vocab_size, scale, time_horizon, min_token, action_dim. 
16:23:33.959 [I] Loaded norm stats from /iris/u/jrpan/openpi/assets/pi0_fast_robomemory_low_mem_finetune/jennypan00/pi0_fast_ft_droid_lerobot (3337674:config.py:170)
16:24:21.932 [I] Initialized data loader:
[0].images['base_0_rgb']: (128, 224, 224, 3)@float32
[0].images['base_1_rgb']: (128, 224, 224, 3)@float32
[0].images['wrist_0_rgb']: (128, 224, 224, 3)@float32
[0].image_masks['base_0_rgb']: (128,)@bool
[0].image_masks['base_1_rgb']: (128,)@bool
[0].image_masks['wrist_0_rgb']: (128,)@bool
[0].state: (128, 8)@float32
[0].tokenized_prompt: (128, 180)@int32
[0].tokenized_prompt_mask: (128, 180)@bool
[0].token_ar_mask: (128, 180)@int32
[0].token_loss_mask: (128, 180)@bool
[1]: (128, 10, 8)@float32 (3337674:train.py:292)
16:24:24.435 [I] Initialized train state:
['PaliGemma']['img']['Transformer']['encoder_norm']['bias'].value: (1152,)@float32
['PaliGemma']['img']['Transformer']['encoder_norm']['scale'].value: (1152,)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_0']['bias'].value: (27, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_0']['scale'].value: (27, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_1']['bias'].value: (27, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_1']['scale'].value: (27, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_0']['bias'].value: (27, 4304)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_0']['kernel'].value: (27, 1152, 4304)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_1']['bias'].value: (27, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_1']['kernel'].value: (27, 4304, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['key']['bias'].value: (27, 16, 72)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['key']['kernel'].value: (27, 1152, 16, 72)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['out']['bias'].value: (27, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['out']['kernel'].value: (27, 16, 72, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['query']['bias'].value: (27, 16, 72)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['query']['kernel'].value: (27, 1152, 16, 72)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['value']['bias'].value: (27, 16, 72)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['value']['kernel'].value: (27, 1152, 16, 72)@float32
['PaliGemma']['img']['embedding']['bias'].value: (1152,)@float32
['PaliGemma']['img']['embedding']['kernel'].value: (14, 14, 3, 1152)@float32
['PaliGemma']['img']['head']['bias'].value: (2048,)@float32
['PaliGemma']['img']['head']['kernel'].value: (1152, 2048)@float32
['PaliGemma']['img']['pos_embedding'].value: (1, 256, 1152)@float32
['PaliGemma']['llm']['embedder']['input_embedding'].value: (257152, 2048)@bfloat16
['PaliGemma']['llm']['final_norm']['scale'].value: (2048,)@bfloat16
['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum']['lora_a'].value: (18, 8, 256, 16)@float32
['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum']['lora_b'].value: (18, 8, 16, 2048)@float32
['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum']['w'].value: (18, 8, 256, 2048)@bfloat16
['PaliGemma']['llm']['layers']['attn']['kv_einsum']['lora_a'].value: (18, 2, 1, 2048, 16)@float32
['PaliGemma']['llm']['layers']['attn']['kv_einsum']['lora_b'].value: (18, 2, 1, 16, 256)@float32
['PaliGemma']['llm']['layers']['attn']['kv_einsum']['w'].value: (18, 2, 1, 2048, 256)@bfloat16
['PaliGemma']['llm']['layers']['attn']['q_einsum']['lora_a'].value: (18, 8, 2048, 16)@float32
['PaliGemma']['llm']['layers']['attn']['q_einsum']['lora_b'].value: (18, 8, 16, 256)@float32
['PaliGemma']['llm']['layers']['attn']['q_einsum']['w'].value: (18, 8, 2048, 256)@bfloat16
['PaliGemma']['llm']['layers']['mlp']['gating_einsum'].value: (18, 2, 2048, 16384)@bfloat16
['PaliGemma']['llm']['layers']['mlp']['gating_einsum_lora_a'].value: (18, 2, 2048, 16)@float32
['PaliGemma']['llm']['layers']['mlp']['gating_einsum_lora_b'].value: (18, 2, 16, 16384)@float32
['PaliGemma']['llm']['layers']['mlp']['linear'].value: (18, 16384, 2048)@bfloat16
['PaliGemma']['llm']['layers']['mlp']['linear_lora_a'].value: (18, 16384, 16)@float32
['PaliGemma']['llm']['layers']['mlp']['linear_lora_b'].value: (18, 16, 2048)@float32
['PaliGemma']['llm']['layers']['pre_attention_norm']['scale'].value: (18, 2048)@bfloat16
['PaliGemma']['llm']['layers']['pre_ffw_norm']['scale'].value: (18, 2048)@bfloat16 (3337674:train.py:303)
16:24:24.440 [I] Restoring checkpoint from /iris/u/jrpan/openpi/checkpoints/pi0_fast_robomemory_low_mem_finetune/lora_10k_0716/2000. (3337674:checkpointer.py:298)
/iris/u/jrpan/openpi/.venv/lib/python3.11/site-packages/orbax/checkpoint/_src/serialization/type_handlers.py:1251: UserWarning: Sharding info not provided when restoring. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.
  warnings.warn(
wandb: WARNING Tried to log to step 0 that is less than the current step 2901. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
16:24:33.023 [I] [process=0] /jax/checkpoint/read/bytes_per_sec: 2.9 GiB/s (total bytes: 25.3 GiB) (time elapsed: 8 seconds) (per-host) (3337674:base_pytree_checkpoint_handler.py:114)
slurmstepd: error: *** JOB 10611858 ON iris-hgx-2 CANCELLED AT 2025-07-16T16:24:35 ***
